{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-6BlM3035mtk"},"outputs":[],"source":["!pip install langchain-core\n","!pip install -U langchain-openai\n","!pip install langgraph"]},{"cell_type":"code","source":["import getpass\n","import os\n","\n","# Get API key from user input and store it in the environment variable.\n","os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n","\n","# Setup environment variable.\n","os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://cccworkshop.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\"\n","os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n","os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n","\n","# Import AzureChatOpenAI\n","from langchain_openai import AzureChatOpenAI\n","\n","# Instantiate the AzureChatOpenAI model, retrieving values from environment variables.\n","model = AzureChatOpenAI(\n","    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n","    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n","    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",")\n","\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","from langchain_core.messages import HumanMessage\n","from langchain_core.messages import AIMessage\n","\n","# Define a new graph\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","\n","# Define the function that calls the model\n","def call_model(state: MessagesState):\n","    response = model.invoke(state[\"messages\"])\n","    return {\"messages\": response}\n","\n","\n","# Define the (single) node in the graph\n","workflow.add_edge(START, \"model\")\n","workflow.add_node(\"model\", call_model)\n","\n","# Add memory\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)\n","\n","# Add threat ID\n","config = {\"configurable\": {\"thread_id\": \"abc123\"}}"],"metadata":{"id":"JlqsEm9x505j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"Imagine you are Professor McGonagall at Hogwarts School of Witchcraft and Wizardry, giving a detailed, magical explanation of Large Language Models (LLMs) to a class of wizards and witches. Go into as much detail as possible, explaining how an LLM learns, how it generates responses, and why it can sometimes be mistaken. Describe it as if it’s a rare and ancient magical artifact or spell, weaving in magical metaphors, Hogwarts lore, and spellbinding explanations that make complex topics enchanting and easy to understand.\"\n","\n","input_messages = [HumanMessage(query)]\n","\n","\n","output = app.invoke({\"messages\": input_messages}, config)\n","output[\"messages\"][-1].pretty_print()\n"],"metadata":{"id":"rS61QfTt6FQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"Imagine you are Professor McGonagall at Hogwarts School of Witchcraft and Wizardry, giving a detailed, magical explanation of Large Language Models (LLMs) to a class of wizards and witches. Go into as much detail as possible, explaining how an LLM learns, how it generates responses, and why it can sometimes be mistaken. Describe it as if it’s a rare and ancient magical artifact or spell, weaving in magical metaphors, Hogwarts lore, and spellbinding explanations that make complex topics enchanting and easy to understand.\"\n","\n","input_messages = [HumanMessage(query)]\n","\n","\n","for chunk, metadata in app.stream(\n","    {\"messages\": input_messages},\n","    config,\n","    stream_mode=\"messages\",\n","):\n","    if isinstance(chunk, AIMessage):  # Filter to just model responses\n","        print(chunk.content, end=\"\")\n","\n"],"metadata":{"id":"5hx0SZgVaapC"},"execution_count":null,"outputs":[]}]}